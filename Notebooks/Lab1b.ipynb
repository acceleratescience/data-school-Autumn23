{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Environments and Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software that you write for all of your research, whether it be data analysis or complex software should be FAIR: Findable, Accessible, Interoperable and Reusable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "It is important for us to separate out projects managing various version of python and package dependencies, etc. There are a couple of popular package and environment managers: conda, poetry, pyenv, to name a few.\n",
    "\n",
    "We're going to keep things basic and use pyenv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "There might be occasions where you need random number generation. For example:\n",
    "- Drawing from probability distributions for simulations\n",
    "- Initializing the weights in a neural network\n",
    "- Shuffling training data\n",
    "\n",
    "Although we call this \"random\", it isn't of course. True randomness is very difficult to generate at the scale of computer hardware. Instead we use pseudo-random number generation.\n",
    "\n",
    "In your programs, there might be a few sources of randomness that you want to control in order to ensure reproducibility. The first of these is the python seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6177528569514706\n",
      "0.5332655736050008\n",
      "Reinitializing the random number generator\n",
      "0.6177528569514706\n",
      "0.5332655736050008\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "print(random.random())\n",
    "print(random.random())\n",
    "print(\"Reinitializing the random number generator...\")\n",
    "random.seed(1337)\n",
    "print(random.random())\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next one is the numpy random number generator. It is common to set the seed using\n",
    "\n",
    "```python\n",
    "np.random.seed(1337)\n",
    "```\n",
    "\n",
    "but the best practice is to use a `Generator` instance instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8781019003471183\n",
      "0.18552796163759344\n",
      "Reinitializing the random number generator...\n",
      "0.8781019003471183\n",
      "0.18552796163759344\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(1337)\n",
    "print(rng.random())\n",
    "print(rng.random())\n",
    "print(\"Reinitializing the random number generator...\")\n",
    "rng = default_rng(1337)\n",
    "print(rng.random())\n",
    "print(rng.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of this is that you can use the random number generator for specific purposes while avoiding any other imported packaged from resetting your global random seed. For most uses, however, using the global method will be OK.\n",
    "\n",
    "Next up, we have sources of randomness from Scikit-Learn. Let's have a look at this...\n",
    "\n",
    "So now we have seen that sklearn just uses the global numpy seed. But let's quickly verify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.random.mtrand.RandomState"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "type(check_random_state(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3884982  -0.5572034   1.2191349  -0.68651964 -0.29221923]\n",
      " [ 0.03002655 -0.38863165  2.03523039  0.28036444 -0.62807045]\n",
      " [-0.18565761  0.20978005 -2.28069864  0.56606969 -0.09268623]\n",
      " [ 0.47270823  0.48015287 -1.54028493 -0.11151901  0.13755263]]\n",
      "[[-0.3884982  -0.5572034   1.2191349  -0.68651964 -0.29221923]\n",
      " [ 0.03002655 -0.38863165  2.03523039  0.28036444 -0.62807045]\n",
      " [-0.18565761  0.20978005 -2.28069864  0.56606969 -0.09268623]\n",
      " [ 0.47270823  0.48015287 -1.54028493 -0.11151901  0.13755263]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "np.random.seed(1337)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 3), random_state=None)\n",
    "clf.fit(X, y)\n",
    "print(clf.coefs_[0])\n",
    "\n",
    "np.random.seed(1337)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 3), random_state=None)\n",
    "clf.fit(X, y)\n",
    "print(clf.coefs_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's super annyoying having to reset the random seed if you want to reproduce results, so we'll have a look at this later. But first, let's tackle PyTorch. There is a lot of nondeterministic features of PyTorch. You can use the torch `manual_seed()` method to fix the RNG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x135438ad0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoyingly, PyTorch operations can sometimes use internal random number generators, so if the operation is called over and over, you'll get different results unless you set the manual seed between calls, like we did with sklearn.\n",
    "\n",
    "In addition, the cuDNN library also can be a source of nondeterminism. This is to do with how cuDNN finds optimal convolution algorithms. You can disable this by using\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "```\n",
    "\n",
    "You can also avoid nondeterministic algorithms, by using\n",
    "\n",
    "```python\n",
    "torch.use_deterministic_algorithms()\n",
    "```\n",
    "\n",
    "This will mess with a lot of potential neural network layers like LSTM and max pooling layers, and probably should be avoided.\n",
    "\n",
    "The short version of the story is it is almost impossible to guarantee absolute reproducibility across all PyTorch versions, on multiple platforms. **In general you should not assume reproducibility between CPU and GPU executions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility from MATLAB to NumPy\n",
    "This is something that might be important to you when verifying algorthims that you are translating from MATLAB to NumPy, and is something that has personally caused me quite a bit of grief.\n",
    "\n",
    "It is possible to exactly reproduce a significant amount of randomness between MATLAB and NumPy. NumPy uses the Mersenne Twister 19937 algorthim by default, and you can force MATLAB to use the same algorithm. This means that both languages will produce the same string of random numbers.\n",
    "\n",
    "Since MATLAB and NumPy also both use the same underlying linear algebra subroutines (BLAS and LAPACK, both written in FORTRAN), you can also reproduce the results of many common linear solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: You should not use these random number generators for security or cryptographic purposes. There are other libraries that exist for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've discussed randomness and reproducibility a little, let's see how we can set these things globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DEFAULT_SEED = 1337\n",
    "\n",
    "def set_python(seed=DEFAULT_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def set_numpy(seed=DEFAULT_SEED):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_torch(seed=DEFAULT_SEED, deterministic=False):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def set_all_seeds(seed=DEFAULT_SEED, deterministic=False):\n",
    "    set_python(seed)\n",
    "    set_numpy(seed)\n",
    "    set_torch(seed, deterministic)\n",
    "\n",
    "set_all_seeds(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not optimize for random seed! Do not base any decisions you make on your random seed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
