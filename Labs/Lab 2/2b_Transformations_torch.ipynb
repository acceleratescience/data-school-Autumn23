{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in PyTorch\n",
    "\n",
    "Now we're going to run the same thing, but using PyTorch, with the aim of using a neural network as our final model. Our final model will be a simple linear model, so nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010,) (337,) (450,)\n"
     ]
    }
   ],
   "source": [
    "# Test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.images, digits.target, test_size=0.25, random_state=1337)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1337)\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Build a basic model with a 2 linear layers, using cross entropy loss and the Adam optimizer all with default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden=32):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(8*8, hidden, dtype=torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 10, dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1944, -1.2189, -0.1181,  2.3252,  0.9390,  0.0453, -0.6722, -0.5918,\n",
       "         -0.3674,  4.4997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(torch.tensor(X_train[0], dtype=torch.float64).unsqueeze(0))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it all works. As a sidenote - testing the output of your neural network layers is a good idea while you're building it just to make sure everything is working as intended.\n",
    "\n",
    "Now, we could just stuff all of our data through the network and train it, and this is fine for small datasets, but as your datasets get larger (which they will do if you're using more complicated NN architectures), then you don't want implement things like batching by hand. Fortunatel PyTorch comes with some pretty great dataloaders that takes care of this for you.\n",
    "\n",
    "In addition, we also want to normalize our data, and convert everything to tensors. Let's try this now.\n",
    "\n",
    "As with the custom sklearn classes, you are required to implement certain methods: `__init__`, `__len__`, and `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]      \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = label\n",
    "        return torch.tensor(image).unsqueeze(0), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a standard min-max scaler in pytorch, so we can implement one ourselves (note that this is overkill - since we know the maximum value and minimum values of the images, we can simply do `X / max(X)`, and the effect will be the same).\n",
    "\n",
    "Again there are certain methods we have to define. The `__call__` method is basically the `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMax(object):\n",
    "    def __init__(self, feature_range=(0,1)):\n",
    "        self.min = feature_range[0]\n",
    "        self.max = feature_range[1]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        std = (sample - np.min(sample)) / (np.max(sample) - np.min(sample))\n",
    "        sample_scaled = std * (self.max - self.min) + self.min\n",
    "\n",
    "        return sample_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We either feed this directly into our dataset, or we can string together multiple transformations using `Compose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    MinMax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DigitsDataset(X_train, y_train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = DigitsDataset(X_val, y_val, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([32, 1, 1, 8, 8]) torch.float64\n",
      "Shape of y: torch.Size([32]) torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/crjxkl596x59j9mv3_2mw1_00000gn/T/ipykernel_99978/3545835507.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image).unsqueeze(0), label\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some functions to run training and testing. These are stock functions shamelessly ripped from the PyTorch tutorials (which are very good by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298777  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 1.748523 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.680736  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.059877 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.973698  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.638599 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.626328  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.462090 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.395452  [   32/ 1010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/crjxkl596x59j9mv3_2mw1_00000gn/T/ipykernel_99978/3545835507.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image).unsqueeze(0), label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.359660 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.381919  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.288526 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.198614  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.256855 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.186870  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.223322 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.154736  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.223808 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.152001  [   32/ 1010]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.180673 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(hidden=256)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, criterion, optimizer)\n",
    "    test(test_dataloader, model, criterion)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, I strongly recommend that you check out the [PyTorch tutorials on custom datasets and transformations](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
